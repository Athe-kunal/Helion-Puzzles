{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc85b658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import torch\n",
    "\n",
    "import helion\n",
    "from helion._testing import DEVICE\n",
    "from helion._testing import run_example\n",
    "import helion.language as hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ce0ae58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing helion correctness...\n",
      "[0s] Autotune random seed: 1806828401\n",
      "[0s] Starting autotuning process, this may take a while...\n",
      "[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/recoverx/astarag/Helion-Puzzles/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/recoverx/astarag/Helion-Puzzles/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[26s] Initial random population of 100, 5 starting points: ok=100 min=0.3768 mid=1.2626 max=71.3226 best=Config(block_sizes=[256, 32], flatten_loops=[True], indexing='pointer', l2_groupings=[4], load_eviction_policies=['', 'last'], loop_orders=[[0, 1]], num_stages=7, num_warps=4, pid_type='persistent_interleaved', range_flattens=[None], range_multi_buffers=[True], range_unroll_factors=[4], range_warp_specializes=[])\n",
      "[26s] Generation 1 starting: 139 neighbors, 5 active search path(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[77s] Generation 1 complete: ok=144 min=0.3758 mid=0.3799 max=21.5281 best=Config(block_sizes=[256, 32], flatten_loops=[True], indexing='pointer', l2_groupings=[4], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=7, num_warps=4, pid_type='persistent_interleaved', range_flattens=[None], range_multi_buffers=[True], range_unroll_factors=[4], range_warp_specializes=[])\n",
      "[77s] Generation 2 starting: 94 neighbors, 4 active search path(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[112s] Generation 2 complete: ok=98 min=0.3758 mid=0.3789 max=5.0668 best=Config(block_sizes=[256, 32], flatten_loops=[True], indexing='pointer', l2_groupings=[4], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=7, num_warps=4, pid_type='persistent_interleaved', range_flattens=[None], range_multi_buffers=[True], range_unroll_factors=[4], range_warp_specializes=[])\n",
      "[112s] Generation 3 starting: 73 neighbors, 3 active search path(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[139s] Generation 3 complete: ok=77 min=0.3758 mid=0.3789 max=19.9250 best=Config(block_sizes=[256, 32], flatten_loops=[True], indexing='pointer', l2_groupings=[4], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=7, num_warps=4, pid_type='persistent_interleaved', range_flattens=[None], range_multi_buffers=[True], range_unroll_factors=[4], range_warp_specializes=[])\n",
      "[139s] Generation 4 starting: 46 neighbors, 2 active search path(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[156s] Generation 4 complete: ok=49 min=0.3758 mid=0.3768 max=20.1600 best=Config(block_sizes=[256, 32], flatten_loops=[True], indexing='pointer', l2_groupings=[4], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=7, num_warps=4, pid_type='persistent_interleaved', range_flattens=[None], range_multi_buffers=[True], range_unroll_factors=[4], range_warp_specializes=[])\n",
      "[156s] Autotuning complete in 156.5s after searching 452 configs.\n",
      "One can hardcode the best config and skip autotuning with:\n",
      "    @helion.kernel(config=helion.Config(block_sizes=[256, 32], flatten_loops=[True], indexing='pointer', l2_groupings=[4], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=7, num_warps=4, pid_type='persistent_interleaved', range_flattens=[None], range_multi_buffers=[True], range_unroll_factors=[4], range_warp_specializes=[]), static_shapes=True)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================================================\n",
      "Benchmark Results\n",
      "=================================================================\n",
      "Implementation       Time (ms)    Speedup        \n",
      "-----------------------------------------------------------------\n",
      "helion               0.3768       1.01x          \n",
      "torch                0.3799       1.00x (ref)    \n",
      "=================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@helion.kernel()\n",
    "def add(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    x, y = torch.broadcast_tensors(x, y)\n",
    "    out = torch.empty_like(x, dtype = torch.promote_types(x.dtype, y.dtype), device=x.device)\n",
    "    for tile in hl.tile(out.size()):\n",
    "        out[tile] = x[tile] + y[tile]\n",
    "    return out\n",
    "\n",
    "def check(m: int, n: int) -> None:\n",
    "    \"\"\"\n",
    "    Verify the add kernel implementation against PyTorch's native add function.\n",
    "\n",
    "    Args:\n",
    "        m: First dimension of the test tensors\n",
    "        n: Second dimension of the test tensors\n",
    "    \"\"\"\n",
    "    x = torch.randn([m, n], device=DEVICE, dtype=torch.float16)\n",
    "    y = torch.randn([m, n], device=DEVICE, dtype=torch.float16)\n",
    "    run_example(add, torch.add, (x, y))\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Main entry point that runs the add kernel verification with 1024x1024 tensors.\n",
    "    \"\"\"\n",
    "    check(10240, 10240)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7d213b",
   "metadata": {},
   "source": [
    "## EXPONENTIAL FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3b832d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@helion.kernel(autotune_effort=\"none\")\n",
    "def exp_fwd(x: torch.Tensor) -> torch.Tensor:\n",
    "    out = torch.empty_like(x)\n",
    "    for tile in hl.tile(out.size()):\n",
    "        out[tile] = torch.exp(x[tile])\n",
    "    return out\n",
    "\n",
    "@helion.kernel(autotune_effort=\"none\")\n",
    "def exp_bwd(dy: torch.Tensor, exp_x: torch.Tensor) -> torch.Tensor:\n",
    "    dx = torch.empty_like(exp_x)\n",
    "    for tile in hl.tile(exp_x.size()):\n",
    "        dx[tile] = dy[tile] * exp_x[tile]\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e05ef164",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx: object, x: torch.Tensor) -> torch.Tensor:\n",
    "        y = exp_fwd(x)\n",
    "        ctx.save_for_backward(y)\n",
    "        return y\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx: object, grad_output: torch.Tensor) -> torch.Tensor:\n",
    "        (x,) = ctx.saved_tensors\n",
    "        return exp_bwd(grad_output, x)\n",
    "    \n",
    "def exp(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Exponential with forward and backward support.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor\n",
    "\n",
    "    Returns:\n",
    "        Output tensor with the exponential of each element in the input\n",
    "    \"\"\"\n",
    "    return ExpFunction.apply(x)  # type: ignore[no-any-return]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f412eef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "def exp_tritonbench(\n",
    "    tb_op: object, x: torch.Tensor\n",
    ") -> Callable[[], dict[str, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Wrapper for tritonbench that returns output in expected format.\n",
    "\n",
    "    Args:\n",
    "        tb_op: TritonBench operator instance\n",
    "        x: Input tensor\n",
    "\n",
    "    Returns:\n",
    "        Callable that returns dictionary containing the output tensor\n",
    "    \"\"\"\n",
    "    return lambda: {\"output\": exp(x)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcc3d40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing helion correctness...\n",
      "Using default config: @helion.kernel(config=helion.Config(block_sizes=[1024], indexing='pointer', load_eviction_policies=[''], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[]), static_shapes=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/recoverx/astarag/Helion-Puzzles/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/recoverx/astarag/Helion-Puzzles/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default config: @helion.kernel(config=helion.Config(block_sizes=[1024], indexing='pointer', load_eviction_policies=['', ''], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[]), static_shapes=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================================================\n",
      "Benchmark Results\n",
      "=================================================================\n",
      "Implementation       Time (ms)    Speedup        \n",
      "-----------------------------------------------------------------\n",
      "helion               0.5110       1.00x          \n",
      "torch                0.5110       1.00x (ref)    \n",
      "=================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def check(n: int) -> None:\n",
    "    \"\"\"\n",
    "    Verify the exp kernel implementation against PyTorch's native exp function.\n",
    "\n",
    "    Args:\n",
    "        n: Size of the test tensor\n",
    "    \"\"\"\n",
    "    x = torch.randn(n, device=DEVICE, dtype=torch.float32, requires_grad=True)\n",
    "    run_example(exp, torch.exp, (x,), bwd=True)\n",
    "    \n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Main entry point that runs the exp kernel verification.\n",
    "    \"\"\"\n",
    "    check(10240 * 10240)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2cfddb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/recoverx/astarag/Helion-Puzzles/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/recoverx/astarag/Helion-Puzzles/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing helion correctness...\n",
      "Using default config: @helion.kernel(config=helion.Config(block_sizes=[1], indexing='pointer', load_eviction_policies=['', ''], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[None]), static_shapes=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/recoverx/astarag/Helion-Puzzles/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/recoverx/astarag/Helion-Puzzles/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================================================\n",
      "Benchmark Results\n",
      "=================================================================\n",
      "Implementation       Time (ms)    Speedup        \n",
      "-----------------------------------------------------------------\n",
      "helion               0.0512       1.08x          \n",
      "torch                0.0553       1.00x (ref)    \n",
      "=================================================================\n",
      "\n",
      "Testing helion correctness...\n",
      "Using default config: @helion.kernel(config=helion.Config(block_sizes=[1], indexing='pointer', load_eviction_policies=['', ''], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[4096]), static_shapes=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================================================\n",
      "Benchmark Results\n",
      "=================================================================\n",
      "Implementation       Time (ms)    Speedup        \n",
      "-----------------------------------------------------------------\n",
      "helion               0.2714       0.99x          \n",
      "torch                0.2693       1.00x (ref)    \n",
      "=================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@helion.kernel(autotune_effort=\"none\")\n",
    "def sum_kernel(x: torch.Tensor) -> torch.Tensor:\n",
    "    m, n = x.shape\n",
    "    out = torch.empty([m], dtype=x.dtype, device=x.device)\n",
    "    \n",
    "    for tile_m in hl.tile(m):\n",
    "        out[tile_m] = x[tile_m,:].sum(-1)\n",
    "    return out\n",
    "\n",
    "def sum_tritonbench(tb_op: object, x: torch.Tensor) -> Callable[[], torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Wrapper for tritonbench that handles 1D input.\n",
    "\n",
    "    Args:\n",
    "        tb_op: TritonBench operator instance\n",
    "        x: Input tensor (1D or 2D)\n",
    "\n",
    "    Returns:\n",
    "        Callable that returns sum of the tensor along the last dimension\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_sum() -> torch.Tensor:\n",
    "        if x.ndim == 1:\n",
    "            # For 1D tensors, reshape to 2D for sum_kernel\n",
    "            x_2d = x.unsqueeze(0)\n",
    "            result = sum_kernel(x_2d)\n",
    "            return result.squeeze()\n",
    "        return sum_kernel(x)\n",
    "\n",
    "    return compute_sum\n",
    "\n",
    "def check(m: int, n: int) -> None:\n",
    "    \"\"\"\n",
    "    Verify the sum kernel implementation against PyTorch's native sum function.\n",
    "\n",
    "    Args:\n",
    "        m: First dimension of the test tensor\n",
    "        n: Second dimension of the test tensor\n",
    "    \"\"\"\n",
    "    x = torch.randn([m, n], device=DEVICE, dtype=torch.float32)\n",
    "    kernels = {\"helion\": sum_kernel}\n",
    "    run_example(kernels, lambda x: x.sum(-1), (x,))\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Main entry point that runs the sum kernel verification with different tensor sizes.\n",
    "    \"\"\"\n",
    "    check(5120, 2560)\n",
    "    check(10240, 10240)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67ccbb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "@helion.kernel(\n",
    "    config=helion.Config(\n",
    "        block_sizes=[32768, 1], num_warps=16, num_stages=5, indexing=\"pointer\"\n",
    "    )\n",
    ")\n",
    "def longsum_manual(x: torch.Tensor) -> torch.Tensor:\n",
    "    m,n = x.size()\n",
    "    out = torch.empty([m], dtype=x.dtype, device=x.device)\n",
    "    \n",
    "    block_size_n = hl.register_block_size(n)\n",
    "    for tile_m in hl.tile(m):\n",
    "        acc = hl.zeros([tile_m, block_size_n], dtype=x.dtype)\n",
    "        for tile_n in hl.tile(n, block_size=block_size_n):\n",
    "            acc += x[tile_m, tile_n]\n",
    "        out[tile_m] = acc.sum(-1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8f5ae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "@helion.kernel()\n",
    "def softmax_two_pass(x: torch.Tensor) -> torch.Tensor:\n",
    "    m, n = x.size()\n",
    "    out = torch.empty_like(x)\n",
    "    block_size_m = hl.register_block_size(m)\n",
    "    block_size_n = hl.register_block_size(n)\n",
    "    for tile_m in hl.tile(m, block_size=block_size_m):\n",
    "        mi = hl.full([tile_m], float(\"-inf\"), dtype=torch.float32)\n",
    "        di = hl.zeros([tile_m], dtype=torch.float32)\n",
    "        for tile_n in hl.tile(n, block_size=block_size_n):\n",
    "            values = x[tile_m, tile_n]\n",
    "            local_amax = torch.amax(values, dim=1)\n",
    "            mi_next = torch.maximum(mi, local_amax)\n",
    "            di = di + torch.exp(mi - mi_next) + torch.exp(\n",
    "                values - mi_next[:,None]\n",
    "            ).sum(dim=1)\n",
    "            mi = mi_next\n",
    "        for tile_n in hl.tile(n, block_size=block_size_n):\n",
    "            values = x[tile_m, tile_n]\n",
    "            out[tile_m, tile_n] = torch.exp(values - mi[:, None]) / di[:, None]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08678d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "helion-puzzles (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
