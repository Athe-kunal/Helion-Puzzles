{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfd50fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import helion\n",
    "import helion.language as hl\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from jaxtyping import Float32, Int32\n",
    "\n",
    "# If you set this to info you will see the output Triton Code\n",
    "logging.getLogger().setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae9ce4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from triton.testing import do_bench\n",
    "def test_kernel(kernel_fn, spec_fn, *args):\n",
    "    \"\"\"Test a Helion kernel against a reference implementation.\"\"\"\n",
    "    # Run our implementation\n",
    "    result = kernel_fn(*args)\n",
    "    # Run reference implementation\n",
    "    expected = spec_fn(*args)\n",
    "\n",
    "    # Check if results match\n",
    "    torch.testing.assert_close(result, expected)\n",
    "    print(\"✅ Results Match ✅\")\n",
    "\n",
    "def benchmark_kernel(kernel_fn, *args, **kwargs):\n",
    "    \"\"\"Benchmark a Helion kernel.\"\"\"\n",
    "    no_args = lambda: kernel_fn(*args, **kwargs)\n",
    "    time_in_ms = do_bench(no_args)\n",
    "    print(f\"⏱ Time: {time_in_ms} ms\")\n",
    "\n",
    "def compare_implementations(kernel_fn, spec_fn, *args, **kwargs):\n",
    "    \"\"\"Benchmark a Helion kernel and its reference implementation.\"\"\"\n",
    "    kernel_no_args = lambda: kernel_fn(*args, **kwargs)\n",
    "    spec_no_args = lambda: spec_fn(*args, **kwargs)\n",
    "    kernel_time = do_bench(kernel_no_args)\n",
    "    spec_time = do_bench(spec_no_args)\n",
    "    print(f\"⏱ Helion Kernel Time: {kernel_time:.3f} ms, PyTorch Reference Time: {spec_time:.3f} ms, Speedup: {spec_time/kernel_time:.3f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479d0456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results Match ✅\n",
      "⏱ Time: 0.006967028159056312 ms\n",
      "⏱ Helion Kernel Time: 0.007 ms, PyTorch Reference Time: 0.006 ms, Speedup: 0.907x\n"
     ]
    }
   ],
   "source": [
    "@helion.kernel(config=helion.Config(block_sizes=[128,128]))\n",
    "def example_add(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    m, n = x.size()\n",
    "    out = torch.empty_like(x)    \n",
    "    for tile_m, tile_n in hl.tile([m,n]):\n",
    "        out[tile_m, tile_n] = x[tile_m, tile_n] + y[tile_m,tile_n]\n",
    "    return out\n",
    "\n",
    "# Create some sample data\n",
    "x = torch.randn(10, 10, device=\"cuda\")\n",
    "y = torch.randn(10, 10, device=\"cuda\")\n",
    "\n",
    "# Run the kernel\n",
    "result = example_add(x, y)\n",
    "\n",
    "# Verify result\n",
    "expected = x + y\n",
    "torch.testing.assert_close(result, expected)\n",
    "print(\"✅ Results Match ✅\")\n",
    "benchmark_kernel(example_add, x, y)\n",
    "compare_implementations(example_add, torch.add, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad25f4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@helion.kernel()\n",
    "def example_add(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    m, n = x.size()\n",
    "    out = torch.empty_like(x)    \n",
    "    for tile_m, tile_n in hl.tile([m,n]):\n",
    "        out[tile_m, tile_n] = x[tile_m, tile_n] + y[tile_m,tile_n]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57a9eba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0s] Autotune random seed: 499402173\n",
      "[0s] Starting autotuning process, this may take a while...\n",
      "[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/recoverx/astarag/Helion-Puzzles/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/recoverx/astarag/Helion-Puzzles/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[25s] Initial random population of 100, 5 starting points: ok=100 min=0.0051 mid=0.0061 max=0.0072 best=Config(block_sizes=[1, 16], flatten_loops=[True], indexing='block_ptr', l2_groupings=[32], load_eviction_policies=['', ''], loop_orders=[[0, 1]], num_stages=2, num_warps=8, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[])\n",
      "[25s] Generation 1 starting: 115 neighbors, 5 active search path(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[55s] Generation 1 complete: ok=120 min=0.0051 mid=0.0072 max=0.0072 best=Config(block_sizes=[2, 16], flatten_loops=[True], indexing='block_ptr', l2_groupings=[32], load_eviction_policies=['', ''], loop_orders=[[0, 1]], num_stages=2, num_warps=8, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[])\n",
      "[55s] Generation 2 starting: 106 neighbors, 5 active search path(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[82s] Generation 2 complete: ok=111 min=0.0061 mid=0.0061 max=0.0072 best=Config(block_sizes=[2, 16], flatten_loops=[True], indexing='block_ptr', l2_groupings=[32], load_eviction_policies=['', ''], loop_orders=[[0, 1]], num_stages=2, num_warps=8, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[])\n",
      "[82s] Autotuning complete in 83.0s after searching 321 configs.\n",
      "One can hardcode the best config and skip autotuning with:\n",
      "    @helion.kernel(config=helion.Config(block_sizes=[2, 16], flatten_loops=[True], indexing='block_ptr', l2_groupings=[32], load_eviction_policies=['', ''], loop_orders=[[0, 1]], num_stages=2, num_warps=8, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[]), static_shapes=True)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results Match ✅\n",
      "⏱ Time: 0.006280533405434754 ms\n",
      "⏱ Helion Kernel Time: 0.006 ms, PyTorch Reference Time: 0.006 ms, Speedup: 1.001x\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10, 10, device=\"cuda\")\n",
    "y = torch.randn(10, 10, device=\"cuda\")\n",
    "\n",
    "# Run the kernel\n",
    "result = example_add(x, y)\n",
    "\n",
    "# Verify result\n",
    "expected = x + y\n",
    "torch.testing.assert_close(result, expected)\n",
    "print(\"✅ Results Match ✅\")\n",
    "benchmark_kernel(example_add, x, y)\n",
    "compare_implementations(example_add, torch.add, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c23dd6",
   "metadata": {},
   "source": [
    "## PUZZLE 1: CONSTANT ADD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0309fd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results Match ✅\n",
      "⏱ Time: 0.01255107654364613 ms\n",
      "⏱ Helion Kernel Time: 0.012 ms, PyTorch Reference Time: 0.006 ms, Speedup: 0.507x\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def add_spec(x: Tensor) -> Tensor:\n",
    "    \"\"\"This is the spec that you should implement in the helion kernel below.\"\"\"\n",
    "    return x + 10.\n",
    "\n",
    "# ---- ✨ Is this the best block size? ----\n",
    "@helion.kernel(config = helion.Config(block_sizes = [1,]))\n",
    "def add_kernel(x: torch.Tensor) -> torch.Tensor:\n",
    "    # ---- ✨ Your Code Here ✨----\n",
    "    # Set up the output buffer which you will return\n",
    "    out = torch.empty_like(x)\n",
    "    n = x.size()[0]\n",
    "    # Use Helion to tile the computation\n",
    "    for tile_n in hl.tile(n):\n",
    "         out[tile_n] = x[tile_n] + 10\n",
    "\n",
    "    return out\n",
    "\n",
    "# Test the kernel\n",
    "x = torch.randn(8192, device=\"cuda\")\n",
    "test_kernel(add_kernel, add_spec, x)\n",
    "benchmark_kernel(add_kernel, x)\n",
    "compare_implementations(add_kernel, add_spec, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "277800b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results Match ✅\n",
      "⏱ Time: 0.01174441568081739 ms\n",
      "⏱ Helion Kernel Time: 0.006 ms, PyTorch Reference Time: 0.006 ms, Speedup: 1.004x\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def add_spec(x: Tensor) -> Tensor:\n",
    "    \"\"\"This is the spec that you should implement in the helion kernel below.\"\"\"\n",
    "    return x + 10.\n",
    "\n",
    "# ---- ✨ Is this the best block size? ----\n",
    "@helion.kernel(config = helion.Config(block_sizes = [128,]))\n",
    "def add_kernel(x: torch.Tensor) -> torch.Tensor:\n",
    "    # ---- ✨ Your Code Here ✨----\n",
    "    # Set up the output buffer which you will return\n",
    "    out = torch.empty_like(x)\n",
    "    n = x.size()[0]\n",
    "    # Use Helion to tile the computation\n",
    "    for tile_n in hl.tile(n):\n",
    "         out[tile_n] = x[tile_n] + 10\n",
    "\n",
    "    return out\n",
    "\n",
    "# Test the kernel\n",
    "x = torch.randn(8192, device=\"cuda\")\n",
    "test_kernel(add_kernel, add_spec, x)\n",
    "benchmark_kernel(add_kernel, x)\n",
    "compare_implementations(add_kernel, add_spec, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068b2c6b",
   "metadata": {},
   "source": [
    "## PUZZLE 2: OUTER VECTOR ADD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea3acef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results Match ✅\n",
      "⏱ Time: 0.007596276778106888 ms\n",
      "⏱ Helion Kernel Time: 0.008 ms, PyTorch Reference Time: 0.009 ms, Speedup: 1.047x\n"
     ]
    }
   ],
   "source": [
    "def broadcast_add_spec(x: Tensor, y: Tensor) -> Tensor:\n",
    "    return x[None, :] + y[:, None]\n",
    "\n",
    "# ---- ✨ Is this the best block size? ----\n",
    "@helion.kernel(config = helion.Config(block_sizes = [32, 32]))\n",
    "def broadcast_add_kernel(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    # Get tensor sizes\n",
    "     # ---- ✨ Your Code Here ✨----\n",
    "    n0 = x.size(0)\n",
    "    n1 = y.size(0)\n",
    "    out = x.new_empty(n1, n0)\n",
    "\n",
    "    # Use Helion to tile the computation\n",
    "    for tile_i, tile_j in hl.tile([n1, n0]):\n",
    "        # Get tiles from x and y\n",
    "        y_tile = y[tile_i]\n",
    "        x_tile = x[tile_j]\n",
    "        # Compute outer sum\n",
    "        out[tile_i, tile_j] = y_tile[:, None] + x_tile[None, :]\n",
    "\n",
    "    return out\n",
    "\n",
    "# Test the kernel\n",
    "x = torch.randn(1142, device=\"cuda\")\n",
    "y = torch.randn(512, device=\"cuda\")\n",
    "test_kernel(broadcast_add_kernel, broadcast_add_spec, x, y)\n",
    "benchmark_kernel(broadcast_add_kernel, x, y)\n",
    "compare_implementations(broadcast_add_kernel, broadcast_add_spec, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8861ace2",
   "metadata": {},
   "source": [
    "## PUZZLE 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eba7dadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results Match ✅\n",
      "⏱ Helion Kernel Time: 0.008 ms, PyTorch Reference Time: 0.011 ms, Speedup: 1.520x\n"
     ]
    }
   ],
   "source": [
    "def mul_relu_block_spec(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.relu(x[None,:]*y[:,None])\n",
    "\n",
    "@helion.kernel(config = helion.Config(block_sizes = [32, 32]))\n",
    "def mul_relu_block_kernel(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    n0 = x.size(0)\n",
    "    n1 = y.size(0)\n",
    "    out = x.new_empty(n1,n0)\n",
    "    \n",
    "    for tile_i, tile_j in hl.tile([n1, n0]):\n",
    "        y_tile = y[tile_i]\n",
    "        x_tile = x[tile_j]\n",
    "        \n",
    "        out[tile_i, tile_j] = torch.relu(x_tile[None,:] * y_tile[:,None])\n",
    "    \n",
    "    return out\n",
    "\n",
    "# Test the kernel\n",
    "x = torch.randn(512, device=\"cuda\")\n",
    "y = torch.randn(512, device=\"cuda\")\n",
    "test_kernel(mul_relu_block_kernel, mul_relu_block_spec, x, y)\n",
    "compare_implementations(mul_relu_block_kernel, mul_relu_block_spec, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "746ee904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results Match ✅\n"
     ]
    }
   ],
   "source": [
    "def mul_relu_block_back_spec(x: Tensor, y: Tensor, dz: Tensor) -> Tensor:\n",
    "    x = x.clone()\n",
    "    y = y.clone()\n",
    "    x = x.requires_grad_(True)\n",
    "    y = y.requires_grad_(True)\n",
    "    z = torch.relu(x * y[:, None])\n",
    "    grad_x, _ = torch.autograd.grad(z, [x,y], dz, retain_graph=True)\n",
    "    return grad_x\n",
    "\n",
    "\n",
    "@helion.kernel(config=helion.Config(block_sizes=[32, 32]))\n",
    "def mul_relu_block_back_kernel(\n",
    "    x: torch.Tensor, y: torch.Tensor, dz: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    # Get tensor sizes\n",
    "    n0 = x.size(1)\n",
    "    n1 = x.size(0)\n",
    "    # Create output tensor for gradients\n",
    "    dx = torch.empty_like(x)\n",
    "\n",
    "    # Use Helion to tile the computation\n",
    "    for tile_i, tile_j in hl.tile([n1, n0]):\n",
    "        # Get input tiles\n",
    "        x_tile = x[tile_i, tile_j]\n",
    "        y_tile = y[tile_i]\n",
    "        dz_tile = dz[tile_i, tile_j]\n",
    "\n",
    "        # Compute gradients for ReLU * multiplication backward\n",
    "        # For ReLU, gradient is 1 where input > 0, 0 otherwise\n",
    "        relu_mask = (x_tile * y_tile[:, None]) > 0\n",
    "        # Chain rule: dx = dz * relu_grad * y\n",
    "        dx[tile_i, tile_j] = dz_tile * relu_mask * y_tile[:, None]\n",
    "\n",
    "    return dx\n",
    "\n",
    "\n",
    "x = torch.randn(512, 1024, device=\"cuda\")\n",
    "y = torch.randn(512, device=\"cuda\")\n",
    "dz = torch.randn(512, 1024, device=\"cuda\")\n",
    "test_kernel(mul_relu_block_back_kernel, mul_relu_block_back_spec, x, y, dz)       \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ce3b14",
   "metadata": {},
   "source": [
    "## LONG SUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae2a9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results Match ✅\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sum_spec(x: Float32[Tensor, \"4 200\"]) -> Float32[Tensor, \"4\"]:\n",
    "    return x.sum(1)\n",
    "\n",
    "@helion.kernel(config=helion.Config(block_sizes=[32, 32]))\n",
    "def sum_kernel(x: torch.Tensor) -> torch.Tensor:\n",
    "    batch, seq_len = x.size()\n",
    "\n",
    "    out = torch.empty(batch, dtype=x.dtype, device=x.device)\n",
    "    for tile_batch in hl.tile(batch):\n",
    "        acc = torch.zeros(tile_batch, dtype=torch.float32, device=x.device)\n",
    "\n",
    "        for tile_seq in hl.tile(seq_len):\n",
    "            chunk = x[tile_batch,tile_seq]\n",
    "            acc += torch.sum(chunk, dim=1)\n",
    "        out[tile_batch] = acc\n",
    "    \n",
    "    return out\n",
    "\n",
    "# Test the kernel\n",
    "x = torch.randn(4, 200, device=\"cuda\")\n",
    "test_kernel(sum_kernel, sum_spec, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17352110",
   "metadata": {},
   "source": [
    "## SOFTMAX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbd01772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results Match ✅\n"
     ]
    }
   ],
   "source": [
    "def softmax_spec(x: Float32[Tensor, \"4 200\"]) -> Float32[Tensor, \"4 200\"]:\n",
    "    x_max = torch.max(x,axis=1, keepdim=True)[0]\n",
    "    x -= x_max\n",
    "    x_exp = x.exp()\n",
    "    return x_exp / x_exp.sum(1,keepdim=True)\n",
    "\n",
    "@helion.kernel(config=helion.Config(block_sizes=[32, 32]))\n",
    "def softmax_kernel(x: torch.Tensor) -> torch.Tensor:\n",
    "    batch, seq_len = x.size()\n",
    "\n",
    "    out = torch.empty_like(x)\n",
    "    block_batch = hl.register_block_size(batch)\n",
    "    block_seq_len = hl.register_block_size(seq_len)\n",
    "\n",
    "    for tile_batch in hl.tile(batch, block_size=block_batch):\n",
    "        _max  = hl.full([tile_batch], float(\"-inf\"), dtype=torch.float32)  \n",
    "        _norm = hl.zeros([tile_batch], dtype=torch.float32)                \n",
    "\n",
    "        for tile_seq in hl.tile(seq_len, block_size=block_seq_len):\n",
    "            chunk_f32 = x[tile_batch, tile_seq].to(torch.float32)          \n",
    "            local_max = torch.amax(chunk_f32, dim=1)                        # [tile_batch]\n",
    "            new_max   = torch.maximum(_max, local_max)                      # [tile_batch]\n",
    "            _norm = _norm * torch.exp(_max - new_max)\n",
    "            _norm = _norm + torch.exp(chunk_f32 - new_max[:, None]).sum(dim=1)\n",
    "            _max  = new_max\n",
    "\n",
    "        # Pass 2: normalize\n",
    "        for tile_seq in hl.tile(seq_len, block_size=block_seq_len):\n",
    "            chunk_f32 = x[tile_batch, tile_seq].to(torch.float32)\n",
    "            out[tile_batch, tile_seq] = (\n",
    "                torch.exp(chunk_f32 - _max[:, None]) / _norm[:, None]\n",
    "            ).to(x.dtype)  # cast back to input dtype\n",
    "\n",
    "    return out\n",
    "\n",
    "# Test the kernel\n",
    "x = torch.randn(4, 200, device=\"cuda\")\n",
    "test_kernel(softmax_kernel, softmax_spec, x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b1cd32",
   "metadata": {},
   "source": [
    "# FLASH ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd370a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results Match ✅\n"
     ]
    }
   ],
   "source": [
    "def flashatt_spec(q: Float32[Tensor, \"200\"], k: Float32[Tensor, \"200\"], v: Float32[Tensor, \"200\"]) -> Float32[Tensor, \"200\"]:\n",
    "    x = q[:, None] * k[None, :]\n",
    "    x_max = x.max(1, keepdim=True)[0]\n",
    "    x = x - x_max\n",
    "    x_exp = x.exp()\n",
    "    soft = x_exp / x_exp.sum(1, keepdim=True)\n",
    "    return (v[None, :] * soft).sum(1)\n",
    "\n",
    "\n",
    "\n",
    "@helion.kernel(config=helion.Config(block_sizes=[32, 32]))\n",
    "def flashatt_kernel(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor) -> torch.Tensor:\n",
    "    # Get tensor size\n",
    "    seq_len = q.size(0)\n",
    "    # Create output tensor\n",
    "    out = torch.empty_like(q)\n",
    "\n",
    "    # Process each query position\n",
    "    for tile_q in hl.tile(seq_len):\n",
    "        q_tile = q[tile_q].to(torch.float32) \n",
    "\n",
    "        # Initialize tracking variables for stable softmax\n",
    "        max_val = torch.full_like(q_tile, float('-inf'))\n",
    "        sum_exp = torch.zeros_like(q_tile)\n",
    "        weighted_sum = torch.zeros_like(q_tile)\n",
    "\n",
    "        # Process in tiles for better cache efficiency\n",
    "        for tile_kv in hl.tile(seq_len):\n",
    "            k_tile = k[tile_kv].to(torch.float32)\n",
    "            v_tile = v[tile_kv].to(torch.float32)\n",
    "\n",
    "            # Compute attention scores\n",
    "            scores = q_tile[:, None] * k_tile[None, :]\n",
    "\n",
    "            # Find max for numerical stability\n",
    "            batch_max = torch.amax(scores, dim=1)\n",
    "            new_max = torch.maximum(max_val, batch_max)\n",
    "\n",
    "            # Scale old accumulations\n",
    "            scale_factor = torch.exp(max_val - new_max)\n",
    "            # correct the previous sum (this is for normalization)\n",
    "            sum_exp = sum_exp * scale_factor\n",
    "            # correct the previous weighted sum (qk with v)\n",
    "            weighted_sum = weighted_sum * scale_factor\n",
    "\n",
    "            # Update with new values\n",
    "            exp_scores = torch.exp(scores - new_max[:, None])\n",
    "            sum_exp = sum_exp + torch.sum(exp_scores, dim=1)\n",
    "            weighted_sum = weighted_sum + torch.sum(exp_scores * v_tile[None, :], dim=1)\n",
    "\n",
    "            # Update max_val\n",
    "            max_val = new_max\n",
    "\n",
    "        # Compute final output\n",
    "        out[tile_q] = weighted_sum / sum_exp\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# Test the kernel\n",
    "q = torch.randn(200, device=\"cuda\")\n",
    "k = torch.randn(200, device=\"cuda\")\n",
    "v = torch.randn(200, device=\"cuda\")\n",
    "test_kernel(flashatt_kernel, flashatt_spec, q, k, v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29073e8",
   "metadata": {},
   "source": [
    "# MATRIX MULTIPLICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3487c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.allow_tf32 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7e5f6634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default config: @helion.kernel(config=helion.Config(block_sizes=[4, 16, 16, 16], indexing='pointer', l2_groupings=[1], load_eviction_policies=['', ''], loop_orders=[[0, 1, 2]], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 0], range_unroll_factors=[0, 0], range_warp_specializes=[]), static_shapes=True)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Tensor-likes are not close!\n\nMismatched elements: 4085 / 4096 (99.7%)\nGreatest absolute difference: 0.021955490112304688 at index (2, 31, 7) (up to 1e-05 allowed)\nGreatest relative difference: 0.48506131768226624 at index (2, 22, 21) (up to 1.3e-06 allowed)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m y = torch.randn(\u001b[32m4\u001b[39m, \u001b[32m32\u001b[39m, \u001b[32m32\u001b[39m, device=\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# y = torch.randn(2, 2, 2, device=\"cuda\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mtest_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdot_kernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdot_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mtest_kernel\u001b[39m\u001b[34m(kernel_fn, spec_fn, *args)\u001b[39m\n\u001b[32m      7\u001b[39m expected = spec_fn(*args)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Check if results match\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtesting\u001b[49m\u001b[43m.\u001b[49m\u001b[43massert_close\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Results Match ✅\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/astarag/Helion-Puzzles/.venv/lib/python3.13/site-packages/torch/testing/_comparison.py:1589\u001b[39m, in \u001b[36massert_close\u001b[39m\u001b[34m(actual, expected, allow_subclasses, rtol, atol, equal_nan, check_device, check_dtype, check_layout, check_stride, msg)\u001b[39m\n\u001b[32m   1567\u001b[39m error_metas = not_close_error_metas(\n\u001b[32m   1568\u001b[39m     actual,\n\u001b[32m   1569\u001b[39m     expected,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1584\u001b[39m     msg=msg,\n\u001b[32m   1585\u001b[39m )\n\u001b[32m   1587\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_metas:\n\u001b[32m   1588\u001b[39m     \u001b[38;5;66;03m# TODO: compose all metas into one AssertionError\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1589\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error_metas[\u001b[32m0\u001b[39m].to_error(msg)\n",
      "\u001b[31mAssertionError\u001b[39m: Tensor-likes are not close!\n\nMismatched elements: 4085 / 4096 (99.7%)\nGreatest absolute difference: 0.021955490112304688 at index (2, 31, 7) (up to 1e-05 allowed)\nGreatest relative difference: 0.48506131768226624 at index (2, 22, 21) (up to 1.3e-06 allowed)"
     ]
    }
   ],
   "source": [
    "def dot_spec(x: Float32[Tensor, \"4 32 32\"], y: Float32[Tensor, \"4 32 32\"]) -> Float32[Tensor, \"4 32 32\"]:\n",
    "    return (x @ y).to(torch.float32)\n",
    "\n",
    "@helion.kernel(autotune_effort=\"none\")\n",
    "def dot_kernel(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    batch, mx, nx = x.size()\n",
    "    _, my, ny = y.size()\n",
    "    assert nx == my, f\"Inner dimensions must match but got {nx=} and {my=}\"\n",
    "    out_type = torch.promote_types(x.dtype, y.dtype)\n",
    "    out = torch.zeros((batch, mx,ny), dtype=out_type,device=x.device)\n",
    "\n",
    "    for tile_batch, tile_x, tile_y in hl.tile([batch, mx, ny]):\n",
    "        acc = hl.zeros([tile_batch, tile_x, tile_y],dtype=out_type)\n",
    "        for tile_k in hl.tile(nx): #or my\n",
    "            x_tile = x[tile_batch, tile_x, tile_k].to(out_type)\n",
    "            y_tile = y[tile_batch, tile_k, tile_y].to(out_type)\n",
    "            acc = torch.baddbmm(acc, x_tile, y_tile)\n",
    "        out[tile_batch, tile_x,tile_y] = acc\n",
    "    return out\n",
    "\n",
    "\n",
    "x = torch.randn(4, 32, 32, device=\"cuda\")\n",
    "# x = torch.randn(2, 2, 2, device=\"cuda\")\n",
    "y = torch.randn(4, 32, 32, device=\"cuda\")\n",
    "# y = torch.randn(2, 2, 2, device=\"cuda\")\n",
    "test_kernel(dot_kernel, dot_spec, x.to(torch.float32), y.to(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "31826484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5123, -0.9325],\n",
       "         [-0.7933,  0.0670]],\n",
       "\n",
       "        [[-0.7205,  0.1165],\n",
       "         [ 1.0029,  0.2240]]], device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_kernel(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fdb465db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5121, -0.9332],\n",
       "         [-0.7939,  0.0670]],\n",
       "\n",
       "        [[-0.7214,  0.1162],\n",
       "         [ 1.0042,  0.2246]]], device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_spec(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb63f633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
